#  Ordinary Least Squares (OLS) Linear Regression â€” From Scratch

This project demonstrates how to implement **Ordinary Least Squares (OLS)** linear regression **without using any external libraries** like NumPy or pandas.  
Itâ€™s a simple and educational example of how linear regression works under the hood.

---

##  Features
- Calculates **slope (m)** and **intercept (b)** manually  
- Uses basic Python operations only  
- Predicts `y` values for given `x` inputs  
- Clean and minimal code â€” great for learning regression fundamentals  

---

##  Formula

The best-fit line is given by:

\[
y = m x + b
\]

\[
x_mean = sum(x)/len(x)
\]


\[
y_mean = sum(y)/len(y)
\]


Where:

\[
m = \frac{\sum (x_i - x_mean)(y_i - y_mean)}{\sum (x_i - x_mean)^2}
\]


\[
b = y_mean - m*x_mean
\]

---
#  Multiple Linear Regression

This project demonstrates **Multiple Linear Regression implemented manually in Python**, without using any external libraries like NumPy or scikit-learn.  
It helps understand the **mathematical logic** behind regression and how the **coefficients are derived step by step** â€” for both types of models:

- **Regression through the Origin**
- **Regression through the Centroid (Normal Regression)**

---

##  Concept Summary

- **Goal:** Predict the dependent variable `Y` using two independent variables, `X1` and `X2`.  
- **Method:** Uses the *least squares method* to minimize the sum of squared errors between actual and predicted values.  
- **Outputs:** Regression coefficients `B0`, `B1`, and `B2`, and the final regression equation.  
- **Types Covered:**
  1. **Through Centroid (Normal Regression)** â†’ includes intercept term `B0`
  2. **Through Origin (No Intercept)** â†’ forces regression to start at (0,0,0)

---

## 1. Regression **Through Centroid** (Normal Regression)

### Model:
\[
Y = B_0 + B_1X_1 + B_2X_2
\]

### Formulas:
\[
B_1 = \frac{Î£(X_2 - \bar{X_2})^2Î£(X_1 - \bar{X_1})(Y - \bar{Y}) - Î£(X_1 - \bar{X_1})(X_2 - \bar{X_2})Î£(X_2 - \bar{X_2})(Y - \bar{Y})}{Î£(X_1 - \bar{X_1})^2Î£(X_2 - \bar{X_2})^2 - [Î£(X_1 - \bar{X_1})(X_2 - \bar{X_2})]^2}
\]

\[
B_2 = \frac{Î£(X_1 - \bar{X_1})^2Î£(X_2 - \bar{X_2})(Y - \bar{Y}) - Î£(X_1 - \bar{X_1})(X_2 - \bar{X_2})Î£(X_1 - \bar{X_1})(Y - \bar{Y})}{Î£(X_1 - \bar{X_1})^2Î£(X_2 - \bar{X_2})^2 - [Î£(X_1 - \bar{X_1})(X_2 - \bar{X_2})]^2}
\]

\[
B_0 = \bar{Y} - B_1\bar{X_1} - B_2\bar{X_2}
\]

 **This is the standard regression model** â€” the regression plane passes through the **centroid** (mean point)  
\((\bar{X_1}, \bar{X_2}, \bar{Y})\).

---

## 2. Regression **Through Origin**

### Model:
\[
Y = B_1X_1 + B_2X_2
\]

### Formulas:
\[
B_1 = \frac{Î£X_2^2Î£X_1Y - Î£X_1X_2Î£X_2Y}{Î£X_1^2Î£X_2^2 - (Î£X_1X_2)^2}
\]

\[
B_2 = \frac{Î£X_1^2Î£X_2Y - Î£X_1X_2Î£X_1Y}{Î£X_1^2Î£X_2^2 - (Î£X_1X_2)^2}
\]

 **Used only when** you are told that the regression line/plane passes **through the origin**,  
i.e., \(Y = 0\) when \(X_1 = X_2 = 0\).  
This version has **no intercept** (`B0 = 0`).

#### When we say a regression passes through the origin, it means the line or plane goes through the point (0, 0, 0), so all the mean are 0. 

---

# Batch Gradient Descent for Linear Regression

Simple from-scratch implementation of **batch gradient descent** to fit a linear model \( y = B_0 + B_1 \cdot x \) using mean squared error.

## Features
-  Computes predictions, loss, and gradients manually  
-  Averages gradients by `n` (standard MSE convention)
-  Prints B0 (intercept), B1 (slope), and loss each iteration
-  Interactive input for data, initial parameters, learning rate

## Mathematical Background

**Cost function**: \( J(B_0, B_1) = \frac{1}{2n} \sum_{i=1}^{n} (y_{\text{pred},i} - y_i)^2 \)

**Gradient updates**:

#  Stochastic Gradient Descent for Linear Regression

###  Overview
This Python project demonstrates **Linear Regression** using **Stochastic Gradient Descent (SGD)** â€” one of the fundamental optimization algorithms in machine learning.

Unlike **Batch Gradient Descent**, which updates parameters after processing the entire dataset, **SGD** updates the model **after each individual data point**.  
This allows faster learning (though sometimes noisier updates), making it a great educational example of how models gradually learn patterns.

---

\[
Y_{pred} = B0 + B1 \times X
\]

Where:
- `B0` â†’ Intercept (bias)
- `B1` â†’ Slope (weight)
- `X` â†’ Input data
- `Y` â†’ Actual output data

---

###  Loss Function

The program minimizes the **Mean Squared Error (MSE)**:

\[
Loss = (Y_{pred} - Y)^2
\]

For each data point, the parameters are updated using the following gradient descent rules:

\[
B0 := B0 - A \times (Y_{pred} - Y)
\]
\[
B1 := B1 - A \times (Y_{pred} - Y) \times X
\]

Where:
- `A` â†’ Learning Rate (step size controlling how fast the model learns)

---

## ðŸ§© Features

 Implements **Stochastic Gradient Descent** step-by-step  
 Shows **parameter updates (B0, B1)** and **loss** after each iteration  
 Interactive user inputs for dataset and hyperparameters  
 Perfect for **students and beginners** learning how gradient descent works  

---

